{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading the Data"
      ],
      "metadata": {
        "id": "4_f7udLjg8q1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Q6OL8vMqCa-H"
      },
      "outputs": [],
      "source": [
        "!wget https://zenodo.org/api/records/4783391/files-archive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip files-archive"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AXel0_tiDP3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!7z x /content/clotho_audio_development.7z\n",
        "!7z x /content/clotho_audio_evaluation.7z\n",
        "!7z x /content/clotho_audio_validation.7z"
      ],
      "metadata": {
        "id": "cxRS63SrDx27",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "Vj_dQh_rfWJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from typing import List, Dict, Tuple, Optional, Union\n",
        "import librosa"
      ],
      "metadata": {
        "id": "Nb2npOjQfaQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioAugmenter:\n",
        "    \"\"\"Audio augmentation class with simple transformation strategies\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 sample_rate: int = 22050,\n",
        "                 time_stretch_range: Tuple[float, float] = (0.9, 1.1),\n",
        "                 pitch_shift_range: Tuple[int, int] = (-2, 2),\n",
        "                 noise_factor_range: Tuple[float, float] = (0.001, 0.005),\n",
        "                 time_mask_param: int = 40,\n",
        "                 freq_mask_param: int = 8,\n",
        "                 n_time_masks: int = 1,\n",
        "                 n_freq_masks: int = 1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sample_rate: Audio sample rate\n",
        "            time_stretch_range: Range for time stretching factor\n",
        "            pitch_shift_range: Range for pitch shift in semitones\n",
        "            noise_factor_range: Range for the factor of gaussian noise to add\n",
        "            time_mask_param: Maximum time mask length\n",
        "            freq_mask_param: Maximum frequency mask length\n",
        "            n_time_masks: Number of time masks to apply\n",
        "            n_freq_masks: Number of frequency masks to apply\n",
        "        \"\"\"\n",
        "        self.sample_rate = sample_rate\n",
        "        self.time_stretch_range = time_stretch_range\n",
        "        self.pitch_shift_range = pitch_shift_range\n",
        "        self.noise_factor_range = noise_factor_range\n",
        "        self.time_mask_param = time_mask_param\n",
        "        self.freq_mask_param = freq_mask_param\n",
        "        self.n_time_masks = n_time_masks\n",
        "        self.n_freq_masks = n_freq_masks\n",
        "\n",
        "        # Initialize time and frequency masking transforms\n",
        "        self.time_masking = T.TimeMasking(time_mask_param=time_mask_param)\n",
        "        self.freq_masking = T.FrequencyMasking(freq_mask_param=freq_mask_param)\n",
        "\n",
        "    def time_stretch(self, waveform: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Apply random time stretching using librosa\"\"\"\n",
        "        stretch_factor = random.uniform(*self.time_stretch_range)\n",
        "\n",
        "        # Convert to numpy for librosa processing\n",
        "        waveform_np = waveform.numpy().squeeze()\n",
        "\n",
        "        try:\n",
        "            # Use librosa's time stretching which works on raw audio\n",
        "            stretched = librosa.effects.time_stretch(waveform_np, rate=stretch_factor)\n",
        "            return torch.tensor(stretched).unsqueeze(0)\n",
        "        except Exception as e:\n",
        "            print(f\"Time stretch augmentation failed: {e}\")\n",
        "            return waveform\n",
        "\n",
        "    def pitch_shift(self, waveform: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Apply random pitch shifting\"\"\"\n",
        "        n_steps = random.randint(*self.pitch_shift_range)\n",
        "\n",
        "        # Convert to numpy for pitch shift\n",
        "        waveform_np = waveform.numpy().squeeze()\n",
        "\n",
        "        try:\n",
        "            # Pitch shift using librosa\n",
        "            shifted = librosa.effects.pitch_shift(\n",
        "                waveform_np,\n",
        "                sr=self.sample_rate,\n",
        "                n_steps=n_steps\n",
        "            )\n",
        "\n",
        "            # Convert back to torch tensor\n",
        "            return torch.tensor(shifted).unsqueeze(0)\n",
        "        except Exception as e:\n",
        "            print(f\"Pitch shift augmentation failed: {e}\")\n",
        "            return waveform\n",
        "\n",
        "    def add_noise(self, waveform: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Add random gaussian noise\"\"\"\n",
        "        noise_factor = random.uniform(*self.noise_factor_range)\n",
        "        noise = torch.randn_like(waveform) * noise_factor\n",
        "        return waveform + noise\n",
        "\n",
        "    def apply_time_freq_mask(self, spectrogram: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Apply time and frequency masking to a spectrogram\"\"\"\n",
        "        aug_spec = spectrogram.clone()\n",
        "\n",
        "        # Apply time masking\n",
        "        for _ in range(self.n_time_masks):\n",
        "            aug_spec = self.time_masking(aug_spec)\n",
        "\n",
        "        # Apply frequency masking\n",
        "        for _ in range(self.n_freq_masks):\n",
        "            aug_spec = self.freq_masking(aug_spec)\n",
        "\n",
        "        return aug_spec\n",
        "\n",
        "    def apply_waveform_augmentations(self, waveform: torch.Tensor, max_length: int,\n",
        "                                    augment_list: List[str] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Apply a list of waveform augmentations and ensure consistent length\"\"\"\n",
        "        if augment_list is None:\n",
        "            return waveform, torch.ones(max_length)\n",
        "\n",
        "        aug_waveform = waveform.clone()\n",
        "\n",
        "        # Apply augmentations with a probability\n",
        "        aug_prob = 0.5\n",
        "\n",
        "        if 'time_stretch' in augment_list and random.random() < aug_prob:\n",
        "            aug_waveform = self.time_stretch(aug_waveform)\n",
        "\n",
        "        if 'pitch_shift' in augment_list and random.random() < aug_prob:\n",
        "            aug_waveform = self.pitch_shift(aug_waveform)\n",
        "\n",
        "        if 'add_noise' in augment_list and random.random() < aug_prob:\n",
        "            aug_waveform = self.add_noise(aug_waveform)\n",
        "\n",
        "        # Ensure consistent length after augmentation\n",
        "        current_length = aug_waveform.shape[1]\n",
        "        mask = torch.ones(max_length)\n",
        "\n",
        "        if current_length > max_length:\n",
        "            # Truncate if longer than max_length\n",
        "            aug_waveform = aug_waveform[:, :max_length]\n",
        "        elif current_length < max_length:\n",
        "            # Pad if shorter than max_length\n",
        "            padding = torch.zeros(1, max_length - current_length)\n",
        "            aug_waveform = torch.cat([aug_waveform, padding], dim=1)\n",
        "            mask[current_length:] = 0\n",
        "\n",
        "        return aug_waveform, mask"
      ],
      "metadata": {
        "id": "-xAGq_tXdrAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClothoBaseDataset(Dataset):\n",
        "    \"\"\"Base class for Clotho datasets with shared functionality\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 base_dir: str,\n",
        "                 split: str = 'train',\n",
        "                 sample_rate: int = 22050,\n",
        "                 max_length_seconds: int = 30,\n",
        "                 augmentations: List[str] = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            base_dir: Root directory containing the Clotho dataset\n",
        "            split: 'train' (combines dev+val) or 'eval'\n",
        "            sample_rate: Target sample rate for audio\n",
        "            max_length_seconds: Maximum length for audio in seconds\n",
        "            augmentations: List of augmentation strategies to apply\n",
        "        \"\"\"\n",
        "        self.base_dir = base_dir\n",
        "        self.split = split\n",
        "        self.sample_rate = sample_rate\n",
        "        self.max_length = int(max_length_seconds * sample_rate)  # Convert seconds to samples\n",
        "        self.augmentations = augmentations if augmentations else []\n",
        "\n",
        "        # Initialize augmenter if needed\n",
        "        if self.augmentations:\n",
        "            self.augmenter = AudioAugmenter(sample_rate=sample_rate)\n",
        "\n",
        "        # For training, merge development and validation\n",
        "        if split == 'train':\n",
        "            # Load and merge caption files\n",
        "            dev_captions = pd.read_csv(os.path.join(base_dir, 'clotho_captions_development.csv'))\n",
        "            val_captions = pd.read_csv(os.path.join(base_dir, 'clotho_captions_validation.csv'))\n",
        "            self.captions_df = pd.concat([dev_captions, val_captions], ignore_index=True)\n",
        "\n",
        "            # Create mapping of file names to their full paths\n",
        "            self.audio_paths = {}\n",
        "            for file in dev_captions['file_name'].unique():\n",
        "                self.audio_paths[file] = os.path.join(base_dir, 'development', file)\n",
        "            for file in val_captions['file_name'].unique():\n",
        "                self.audio_paths[file] = os.path.join(base_dir, 'validation', file)\n",
        "\n",
        "        else:  # Evaluation split for validation/testing\n",
        "            self.captions_df = pd.read_csv(os.path.join(base_dir, 'clotho_captions_evaluation.csv'))\n",
        "            self.audio_paths = {\n",
        "                file: os.path.join(base_dir, 'evaluation', file)\n",
        "                for file in self.captions_df['file_name'].unique()\n",
        "            }\n",
        "\n",
        "        # Prepare file list - get unique file names\n",
        "        self.file_list = list(self.audio_paths.keys())\n",
        "        print(f\"Loaded {len(self.file_list)} unique audio files for {split}\")\n",
        "        print(f\"Using max length of {max_length_seconds} seconds ({self.max_length} samples)\")\n",
        "\n",
        "        if self.augmentations:\n",
        "            print(f\"Using audio augmentations: {', '.join(self.augmentations)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def _load_and_process_audio(self, file_name: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Load audio file and handle variable length\"\"\"\n",
        "        audio_path = self.audio_paths[file_name]\n",
        "\n",
        "        try:\n",
        "            # Load audio\n",
        "            waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "            # Convert to mono if needed\n",
        "            if waveform.shape[0] > 1:\n",
        "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "            # Resample if needed\n",
        "            if sample_rate != self.sample_rate:\n",
        "                resampler = T.Resample(sample_rate, self.sample_rate)\n",
        "                waveform = resampler(waveform)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading audio file {file_name}: {e}\")\n",
        "            # Create empty audio as fallback\n",
        "            waveform = torch.zeros(1, self.max_length)\n",
        "\n",
        "        # Handle variable length\n",
        "        original_length = waveform.shape[1]\n",
        "\n",
        "        # Create attention mask (1 for real data, 0 for padding)\n",
        "        mask = torch.ones(self.max_length)\n",
        "\n",
        "        if original_length > self.max_length:\n",
        "            # Truncate\n",
        "            waveform = waveform[:, :self.max_length]\n",
        "            mask[:] = 1\n",
        "        else:\n",
        "            # Pad\n",
        "            padding = torch.zeros(1, self.max_length - original_length)\n",
        "            waveform = torch.cat([waveform, padding], dim=1)\n",
        "            mask[original_length:] = 0\n",
        "\n",
        "        # Apply waveform augmentations if in training mode\n",
        "        if self.split == 'train' and hasattr(self, 'augmenter') and self.augmentations:\n",
        "            waveform_aug_list = [aug for aug in self.augmentations\n",
        "                              if aug in ['time_stretch', 'pitch_shift', 'add_noise']]\n",
        "            if waveform_aug_list:\n",
        "                # Note: augmenter now returns both the waveform and updated mask\n",
        "                waveform, mask = self.augmenter.apply_waveform_augmentations(\n",
        "                    waveform, self.max_length, waveform_aug_list\n",
        "                )\n",
        "\n",
        "        return waveform, mask\n",
        "\n",
        "    def _get_captions(self, file_name: str) -> List[str]:\n",
        "        \"\"\"Retrieve all captions for a given file\"\"\"\n",
        "        file_captions = self.captions_df[self.captions_df['file_name'] == file_name]\n",
        "        caption_list = []\n",
        "        for i in range(1, 6):  # 5 captions per file\n",
        "            col_name = f'caption_{i}'\n",
        "            if col_name in file_captions.columns:\n",
        "                if not file_captions.empty:\n",
        "                    caption = file_captions[col_name].iloc[0]\n",
        "                    caption_list.append(caption)\n",
        "\n",
        "        # If no captions found, add placeholder\n",
        "        if not caption_list:\n",
        "            caption_list = [\"No caption available\"] * 5\n",
        "\n",
        "        return caption_list\n",
        "\n",
        "    def collate_fn(self, batch: List[Dict]) -> Dict:\n",
        "        \"\"\"Custom collate function for handling variable length sequences\"\"\"\n",
        "        audio = torch.stack([item['audio'] for item in batch])\n",
        "        features = [item['features'] for item in batch]\n",
        "        masks = torch.stack([item['mask'] for item in batch])\n",
        "        file_names = [item['file_name'] for item in batch]\n",
        "\n",
        "        # Each item has a list of 5 captions\n",
        "        captions_list = [item['captions'] for item in batch]\n",
        "\n",
        "        captions = [random.choice(captions) for captions in captions_list]\n",
        "\n",
        "        if isinstance(features[0], list):\n",
        "            # For multiscale, we have a list of tensors for each item\n",
        "            collated_features = []\n",
        "            for scale_idx in range(len(features[0])):\n",
        "                scale_features = torch.stack([item[scale_idx] for item in features])\n",
        "                collated_features.append(scale_features)\n",
        "        else:\n",
        "            # Regular spectrogram - stack all features\n",
        "            collated_features = torch.stack(features)\n",
        "\n",
        "        return {\n",
        "            'audio': audio,\n",
        "            'features': collated_features,\n",
        "            'masks': masks,\n",
        "            'file_names': file_names,\n",
        "            'captions': captions,\n",
        "            'all_captions': captions_list\n",
        "        }"
      ],
      "metadata": {
        "id": "sP6ch9SxfnHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogMelSpectrogramDataset(ClothoBaseDataset):\n",
        "    \"\"\"Clotho dataset with standard log-mel spectrogram features\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 base_dir: str,\n",
        "                 split: str = 'train',\n",
        "                 sample_rate: int = 22050,\n",
        "                 max_length_seconds: int = 30,\n",
        "                 n_fft: int = 1024,\n",
        "                 hop_length: int = 512,\n",
        "                 n_mels: int = 64,\n",
        "                 augmentations: List[str] = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            base_dir: Root directory containing the Clotho dataset\n",
        "            split: 'train' (combines dev+val) or 'eval'\n",
        "            sample_rate: Target sample rate for audio\n",
        "            max_length_seconds: Maximum length for audio in seconds\n",
        "            n_fft: FFT size\n",
        "            hop_length: Hop length for STFT\n",
        "            n_mels: Number of mel bands\n",
        "            augmentations: List of augmentation strategies to apply\n",
        "        \"\"\"\n",
        "        super().__init__(base_dir, split, sample_rate, max_length_seconds, augmentations)\n",
        "\n",
        "        # Setup feature extraction\n",
        "        self.feature_extractor = T.MelSpectrogram(\n",
        "            sample_rate=self.sample_rate,\n",
        "            n_fft=n_fft,\n",
        "            hop_length=hop_length,\n",
        "            n_mels=n_mels\n",
        "        )\n",
        "        self.to_db = T.AmplitudeToDB()\n",
        "\n",
        "        print(f\"Using log-mel spectrogram with n_fft={n_fft}, hop_length={hop_length}, n_mels={n_mels}\")\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict:\n",
        "        file_name = self.file_list[idx]\n",
        "\n",
        "        # Load and process audio\n",
        "        waveform, mask = self._load_and_process_audio(file_name)\n",
        "\n",
        "        # Extract features\n",
        "        features = self.feature_extractor(waveform)\n",
        "        features = self.to_db(features)\n",
        "\n",
        "        # Apply spectrogram augmentations if in training mode\n",
        "        if self.split == 'train' and hasattr(self, 'augmenter') and self.augmentations:\n",
        "            spec_aug_list = [aug for aug in self.augmentations\n",
        "                            if aug in ['time_mask', 'freq_mask']]\n",
        "            if spec_aug_list:\n",
        "                features = self.augmenter.apply_time_freq_mask(features)\n",
        "\n",
        "        # Get captions\n",
        "        caption_list = self._get_captions(file_name)\n",
        "\n",
        "        return {\n",
        "            'audio': waveform,\n",
        "            'features': features,\n",
        "            'mask': mask,\n",
        "            'file_name': file_name,\n",
        "            'captions': caption_list\n",
        "        }"
      ],
      "metadata": {
        "id": "_gYGTxlCfw7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiscaleLogMelDataset(ClothoBaseDataset):\n",
        "    \"\"\"Clotho dataset with multiscale log-mel spectrogram features\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 base_dir: str,\n",
        "                 split: str = 'train',\n",
        "                 sample_rate: int = 22050,\n",
        "                 max_length_seconds: int = 30,\n",
        "                 n_fft_scales: List[int] = [512, 1024, 2048],\n",
        "                 hop_length_scales: List[int] = [256, 512, 1024],\n",
        "                 n_mels: int = 64,\n",
        "                 augmentations: List[str] = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            base_dir: Root directory containing the Clotho dataset\n",
        "            split: 'train' (combines dev+val) or 'eval'\n",
        "            sample_rate: Target sample rate for audio\n",
        "            max_length_seconds: Maximum length for audio in seconds\n",
        "            n_fft_scales: List of FFT sizes for different scales\n",
        "            hop_length_scales: List of hop lengths for different scales\n",
        "            n_mels: Number of mel bands\n",
        "            augmentations: List of augmentation strategies to apply\n",
        "        \"\"\"\n",
        "        super().__init__(base_dir, split, sample_rate, max_length_seconds, augmentations)\n",
        "\n",
        "        assert len(n_fft_scales) == len(hop_length_scales), \"Number of FFT scales must match hop length scales\"\n",
        "\n",
        "        # Setup feature extractors for each scale\n",
        "        self.feature_extractors = []\n",
        "\n",
        "        for n_fft, hop_length in zip(n_fft_scales, hop_length_scales):\n",
        "            extractor = T.MelSpectrogram(\n",
        "                sample_rate=self.sample_rate,\n",
        "                n_fft=n_fft,\n",
        "                hop_length=hop_length,\n",
        "                n_mels=n_mels\n",
        "            )\n",
        "            self.feature_extractors.append(extractor)\n",
        "\n",
        "        self.to_db = T.AmplitudeToDB()\n",
        "        self.n_scales = len(n_fft_scales)\n",
        "\n",
        "        print(f\"Using {self.n_scales} spectral scales with FFT sizes: {n_fft_scales} and hop lengths: {hop_length_scales}\")\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict:\n",
        "        file_name = self.file_list[idx]\n",
        "\n",
        "        # Load and process audio\n",
        "        waveform, mask = self._load_and_process_audio(file_name)\n",
        "\n",
        "        # Extract features at multiple scales\n",
        "        multiscale_features = []\n",
        "        for extractor in self.feature_extractors:\n",
        "            features = extractor(waveform)\n",
        "            features = self.to_db(features)\n",
        "\n",
        "            # Apply spectrogram augmentations if in training mode\n",
        "            if self.split == 'train' and hasattr(self, 'augmenter') and self.augmentations:\n",
        "                spec_aug_list = [aug for aug in self.augmentations\n",
        "                                if aug in ['time_mask', 'freq_mask']]\n",
        "                if spec_aug_list:\n",
        "                    features = self.augmenter.apply_time_freq_mask(features)\n",
        "\n",
        "            multiscale_features.append(features)\n",
        "\n",
        "        # Get captions\n",
        "        caption_list = self._get_captions(file_name)\n",
        "\n",
        "        return {\n",
        "            'audio': waveform,\n",
        "            'features': multiscale_features,  # List of features at different scales\n",
        "            'mask': mask,\n",
        "            'file_name': file_name,\n",
        "            'captions': caption_list\n",
        "        }"
      ],
      "metadata": {
        "id": "V4PmqnNdf4tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content\"\n",
        "augmentations = ['time_stretch', 'pitch_shift', 'add_noise', 'time_mask', 'freq_mask']\n",
        "\n",
        "# Test standard log-mel dataset\n",
        "print(\"\\nTesting Standard Log-Mel Dataset:\")\n",
        "standard_dataset = LogMelSpectrogramDataset(\n",
        "    base_dir=base_dir,\n",
        "    split='train',\n",
        "    max_length_seconds=30,\n",
        "    augmentations=augmentations\n",
        ")\n",
        "\n",
        "# Create dataloader\n",
        "standard_loader = DataLoader(\n",
        "    standard_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=standard_dataset.collate_fn\n",
        ")\n",
        "\n",
        "# Get a batch\n",
        "batch = next(iter(standard_loader))\n",
        "print(f\"Audio batch shape: {batch['audio'].shape}\")\n",
        "print(f\"Features shape: {batch['features'].shape}\")\n",
        "print(f\"Mask shape: {batch['masks'].shape}\")\n",
        "print(f\"First caption: {batch['captions'][0]}\")\n",
        "print()\n",
        "\n",
        "# Test multiscale log-mel dataset\n",
        "print(\"\\nTesting Multiscale Log-Mel Dataset:\")\n",
        "multiscale_dataset = MultiscaleLogMelDataset(\n",
        "    base_dir=base_dir,\n",
        "    split='train',\n",
        "    max_length_seconds=30,\n",
        "    n_fft_scales=[512, 1024, 2048],\n",
        "    hop_length_scales=[256, 512, 1024],\n",
        "    augmentations=augmentations\n",
        ")\n",
        "\n",
        "# Create dataloader\n",
        "multiscale_loader = DataLoader(\n",
        "    multiscale_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=multiscale_dataset.collate_fn\n",
        ")\n",
        "\n",
        "# Get a batch\n",
        "batch = next(iter(multiscale_loader))\n",
        "print(f\"Audio batch shape: {batch['audio'].shape}\")\n",
        "print(f\"Number of feature scales: {len(batch['features'])}\")\n",
        "\n",
        "for i, scale_features in enumerate(batch['features']):\n",
        "    print(f\"Features scale {i} shape: {scale_features.shape}\")\n",
        "\n",
        "print(f\"Mask shape: {batch['masks'].shape}\")\n",
        "print(f\"First caption: {batch['captions'][0]}\")"
      ],
      "metadata": {
        "id": "kA_Q0ncHgDzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Functions"
      ],
      "metadata": {
        "id": "2aKj9QbMoRam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class InfoNCELoss(nn.Module):\n",
        "    \"\"\"InfoNCE loss for contrastive learning between audio and text embeddings.\"\"\"\n",
        "    def __init__(self, temperature=0.07):\n",
        "        super(InfoNCELoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, audio_embeddings, text_embeddings):\n",
        "        \"\"\"\n",
        "        Calculate the InfoNCE loss.\n",
        "\n",
        "        Args:\n",
        "            audio_embeddings: Tensor of shape [batch_size, embedding_dim]\n",
        "            text_embeddings: Tensor of shape [batch_size, embedding_dim]\n",
        "\n",
        "        Returns:\n",
        "            loss: The InfoNCE loss value\n",
        "        \"\"\"\n",
        "        # Normalize embeddings for cosine similarity\n",
        "        audio_embeddings = F.normalize(audio_embeddings, dim=1)\n",
        "        text_embeddings = F.normalize(text_embeddings, dim=1)\n",
        "\n",
        "        # Calculate similarity matrix\n",
        "        similarity_matrix = torch.matmul(text_embeddings, audio_embeddings.T) / self.temperature\n",
        "\n",
        "        # Labels are the diagonal indices (matching pairs)\n",
        "        batch_size = audio_embeddings.shape[0]\n",
        "        labels = torch.arange(batch_size).to(audio_embeddings.device)\n",
        "\n",
        "        # Calculate loss in both directions (text-to-audio and audio-to-text)\n",
        "        loss_t2a = self.criterion(similarity_matrix, labels)  # text as query, audio as target\n",
        "        loss_a2t = self.criterion(similarity_matrix.T, labels)  # audio as query, text as target\n",
        "\n",
        "        # Total loss is the average of both directions\n",
        "        loss = (loss_t2a + loss_a2t) / 2.0\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "hCoG1O8UoQai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VICRegLoss(nn.Module):\n",
        "    \"\"\"VICReg loss (Variance-Invariance-Covariance Regularization)\"\"\"\n",
        "    def __init__(self, sim_weight=25.0, var_weight=25.0, cov_weight=1.0, epsilon=1e-4):\n",
        "        super(VICRegLoss, self).__init__()\n",
        "        self.sim_weight = sim_weight\n",
        "        self.var_weight = var_weight\n",
        "        self.cov_weight = cov_weight\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, audio_embeddings, text_embeddings):\n",
        "        \"\"\"\n",
        "        Calculate the VICReg loss between audio and text embeddings.\n",
        "\n",
        "        Args:\n",
        "            audio_embeddings: Tensor of shape [batch_size, embedding_dim]\n",
        "            text_embeddings: Tensor of shape [batch_size, embedding_dim]\n",
        "\n",
        "        Returns:\n",
        "            loss: The VICReg loss value\n",
        "        \"\"\"\n",
        "        # Invariance/similarity loss (MSE between paired embeddings)\n",
        "        sim_loss = F.mse_loss(audio_embeddings, text_embeddings)\n",
        "\n",
        "        # Center the embeddings\n",
        "        audio_embeddings_centered = audio_embeddings - audio_embeddings.mean(dim=0)\n",
        "        text_embeddings_centered = text_embeddings - text_embeddings.mean(dim=0)\n",
        "\n",
        "        # Variance loss (ensures representations have variance above threshold)\n",
        "        audio_std = torch.sqrt(audio_embeddings_centered.var(dim=0) + self.epsilon)\n",
        "        text_std = torch.sqrt(text_embeddings_centered.var(dim=0) + self.epsilon)\n",
        "\n",
        "        audio_var_loss = torch.mean(F.relu(1.0 - audio_std))\n",
        "        text_var_loss = torch.mean(F.relu(1.0 - text_std))\n",
        "        var_loss = audio_var_loss + text_var_loss\n",
        "\n",
        "        # Covariance loss (decorrelates dimensions)\n",
        "        batch_size = audio_embeddings.shape[0]\n",
        "        embedding_dim = audio_embeddings.shape[1]\n",
        "\n",
        "        audio_cov = (audio_embeddings_centered.T @ audio_embeddings_centered) / (batch_size - 1)\n",
        "        text_cov = (text_embeddings_centered.T @ text_embeddings_centered) / (batch_size - 1)\n",
        "\n",
        "        # Zero out the diagonal elements (self-correlation)\n",
        "        audio_cov_off_diag = audio_cov - torch.diag(torch.diag(audio_cov))\n",
        "        text_cov_off_diag = text_cov - torch.diag(torch.diag(text_cov))\n",
        "\n",
        "        # Square and sum off-diagonal elements\n",
        "        audio_cov_loss = torch.sum(audio_cov_off_diag ** 2) / embedding_dim\n",
        "        text_cov_loss = torch.sum(text_cov_off_diag ** 2) / embedding_dim\n",
        "\n",
        "        cov_loss = audio_cov_loss + text_cov_loss\n",
        "\n",
        "        # Combine the three loss components\n",
        "        loss = self.sim_weight * sim_loss + self.var_weight * var_loss + self.cov_weight * cov_loss\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "epIUX9lIoYXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CosineLoss(nn.Module):\n",
        "    \"\"\"Cosine similarity loss for matching audio and text embeddings.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(CosineLoss, self).__init__()\n",
        "\n",
        "    def forward(self, audio_embeddings, text_embeddings):\n",
        "        \"\"\"\n",
        "        Calculate the cosine loss.\n",
        "\n",
        "        Args:\n",
        "            audio_embeddings: Tensor of shape [batch_size, embedding_dim]\n",
        "            text_embeddings: Tensor of shape [batch_size, embedding_dim]\n",
        "\n",
        "        Returns:\n",
        "            loss: The cosine loss value\n",
        "        \"\"\"\n",
        "        # Normalize embeddings for cosine similarity\n",
        "        audio_embeddings = F.normalize(audio_embeddings, dim=1)\n",
        "        text_embeddings = F.normalize(text_embeddings, dim=1)\n",
        "\n",
        "        # Calculate cosine similarity for the positive pairs\n",
        "        batch_size = audio_embeddings.shape[0]\n",
        "        similarity_matrix = torch.matmul(text_embeddings, audio_embeddings.T)\n",
        "        pos_indices = torch.arange(batch_size).to(audio_embeddings.device)\n",
        "        positive_similarities = similarity_matrix[pos_indices, pos_indices]\n",
        "\n",
        "        # Negative because we want to maximize similarity (minimize negative similarity)\n",
        "        loss = -torch.mean(positive_similarities)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "E3feOFSwofJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Encoder"
      ],
      "metadata": {
        "id": "eSiy2pJDhJ9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "\n",
        "class BertTextEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Text encoder using a frozen pre-trained BERT model.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 bert_model_name=\"bert-base-uncased\",\n",
        "                 output_dim=512,\n",
        "                 pooling_strategy=\"cls\"):\n",
        "        \"\"\"\n",
        "        Initialize the BERT text encoder.\n",
        "\n",
        "        Args:\n",
        "            bert_model_name: Pre-trained BERT model name\n",
        "            output_dim: Dimension of the output embedding\n",
        "            pooling_strategy: Strategy to pool token embeddings (\"cls\", \"mean\")\n",
        "        \"\"\"\n",
        "        super(BertTextEncoder, self).__init__()\n",
        "\n",
        "        # Initialize BERT model and tokenizer\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "        self.pooling_strategy = pooling_strategy\n",
        "\n",
        "        # Freeze BERT parameters\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Get BERT hidden dimension\n",
        "        self.bert_dim = self.bert.config.hidden_size\n",
        "\n",
        "        # Projection layer if output_dim is different from BERT hidden size\n",
        "        self.use_projection = (output_dim != self.bert_dim)\n",
        "        if self.use_projection:\n",
        "            self.projection = nn.Linear(self.bert_dim, output_dim)\n",
        "\n",
        "    def forward(self, captions):\n",
        "        \"\"\"\n",
        "        Process text captions through BERT.\n",
        "\n",
        "        Args:\n",
        "            captions: List of caption strings\n",
        "\n",
        "        Returns:\n",
        "            embeddings: Tensor of shape [batch_size, output_dim]\n",
        "        \"\"\"\n",
        "        # Tokenize captions\n",
        "        encoding = self.tokenizer(\n",
        "            captions,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=77,  # Standard max length for many models\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(next(self.bert.parameters()).device)\n",
        "\n",
        "        # Pass through BERT\n",
        "        with torch.no_grad():  # No gradients since BERT is frozen\n",
        "            outputs = self.bert(**encoding)\n",
        "\n",
        "        # Get embeddings based on pooling strategy\n",
        "        if self.pooling_strategy == \"cls\":\n",
        "            # Use CLS token embedding\n",
        "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "        elif self.pooling_strategy == \"mean\":\n",
        "            # Mean pooling over tokens\n",
        "            # Create attention mask (1 for tokens, 0 for padding)\n",
        "            attention_mask = encoding['attention_mask']\n",
        "            # Mean pooling over non-padding tokens\n",
        "            token_embeddings = outputs.last_hidden_state\n",
        "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "            embeddings = sum_embeddings / sum_mask\n",
        "\n",
        "        # Project to desired output dimension if needed\n",
        "        if self.use_projection:\n",
        "            embeddings = self.projection(embeddings)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def encode_text(self, captions):\n",
        "        \"\"\"\n",
        "        Public method to encode text for inference.\n",
        "\n",
        "        Args:\n",
        "            captions: List of caption strings\n",
        "\n",
        "        Returns:\n",
        "            embeddings: Tensor of shape [batch_size, output_dim]\n",
        "        \"\"\"\n",
        "        return self.forward(captions)"
      ],
      "metadata": {
        "id": "jG8BTEb7kX9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Encoder"
      ],
      "metadata": {
        "id": "SSve6pAg35QQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioEncoderCRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    CRNN-based audio encoder for single or multi-scale mel spectrograms.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        output_dim=512,\n",
        "        cnn_channels=[64, 128, 256, 512],\n",
        "        kernel_size=3,\n",
        "        stride=2,\n",
        "        gru_hidden_size=512,\n",
        "        gru_num_layers=2,\n",
        "        dropout_rate=0.3,\n",
        "        use_multiscale=False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the CRNN audio encoder.\n",
        "\n",
        "        Args:\n",
        "            output_dim: Final embedding dimension\n",
        "            cnn_channels: List of channel dimensions for CNN layers\n",
        "            kernel_size: Kernel size for CNN layers\n",
        "            stride: Stride for CNN layers\n",
        "            gru_hidden_size: Hidden size for GRU\n",
        "            gru_num_layers: Number of GRU layers\n",
        "            dropout_rate: Dropout rate\n",
        "            use_multiscale: Whether to use multiscale input processing\n",
        "        \"\"\"\n",
        "        super(AudioEncoderCRNN, self).__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.use_multiscale = use_multiscale\n",
        "\n",
        "        # CNN for feature extraction (same architecture for all scales)\n",
        "        cnn_layers = []\n",
        "        in_channels = 1  # Mel spectrograms have 1 channel\n",
        "\n",
        "        for i, out_channels in enumerate(cnn_channels):\n",
        "            # Add convolutional block with batch norm and dropout\n",
        "            cnn_layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                              stride=stride, padding=kernel_size//2),\n",
        "                    nn.BatchNorm2d(out_channels),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout2d(dropout_rate)\n",
        "                )\n",
        "            )\n",
        "            in_channels = out_channels\n",
        "\n",
        "        self.cnn = nn.Sequential(*cnn_layers)\n",
        "\n",
        "        # Add a projection layer to handle the CNN output to GRU input conversion\n",
        "        # This is flexible and will adapt to whatever dimensions come out of the CNN\n",
        "        self.projection = nn.Linear(cnn_channels[-1], cnn_channels[-1])\n",
        "\n",
        "        # GRU for sequence modeling\n",
        "        self.gru_input_size = cnn_channels[-1]  # This stays the same\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=self.gru_input_size,\n",
        "            hidden_size=gru_hidden_size,\n",
        "            num_layers=gru_num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout_rate if gru_num_layers > 1 else 0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Final FC layers with dropout\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(gru_hidden_size * 2, 768),  # *2 for bidirectional\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(768, output_dim)\n",
        "        )\n",
        "\n",
        "    def _process_single_scale(self, x):\n",
        "        \"\"\"Process a single-scale spectrogram.\"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # CNN feature extraction\n",
        "        x = self.cnn(x)  # (B, C, H', W')\n",
        "\n",
        "        # Use adaptive pooling to handle variable height dimension\n",
        "        # This collapses the height dimension to 1\n",
        "        x = F.adaptive_avg_pool2d(x, (1, x.size(3)))  # (B, C, 1, W')\n",
        "\n",
        "        # Remove height dimension and transpose to (B, W', C)\n",
        "        x = x.squeeze(2).permute(0, 2, 1)\n",
        "\n",
        "        # Now x is of shape (B, W', C) where C = cnn_channels[-1] = 512\n",
        "        # This matches what the GRU expects\n",
        "\n",
        "        # RNN sequence modeling\n",
        "        self.gru.flatten_parameters()\n",
        "        x, _ = self.gru(x)  # (B, W', hidden_size*2)\n",
        "\n",
        "        # Take the last time step output\n",
        "        x = x[:, -1, :]\n",
        "\n",
        "        # FC layers\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Process audio features through the CRNN.\n",
        "\n",
        "        Args:\n",
        "            x: For standard dataset: Tensor of shape [batch_size, 1, height, width]\n",
        "               For multiscale dataset: List of tensors, each of shape [batch_size, 1, height, width]\n",
        "\n",
        "        Returns:\n",
        "            embeddings: Tensor of shape [batch_size, output_dim]\n",
        "        \"\"\"\n",
        "        if self.use_multiscale:\n",
        "            # Process each scale separately\n",
        "            if not isinstance(x, list):\n",
        "                raise ValueError(\"Expected a list of tensors for multiscale input\")\n",
        "\n",
        "            scale_embeddings = []\n",
        "            for scale_x in x:\n",
        "                scale_embedding = self._process_single_scale(scale_x)\n",
        "                scale_embeddings.append(scale_embedding)\n",
        "\n",
        "            # Average the embeddings from different scales\n",
        "            embeddings = torch.stack(scale_embeddings, dim=0).mean(dim=0)\n",
        "        else:\n",
        "            # Process single-scale input\n",
        "            embeddings = self._process_single_scale(x)\n",
        "\n",
        "        # Normalize output embeddings\n",
        "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "T1d0zoK6sIw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Function"
      ],
      "metadata": {
        "id": "CNp0ZHOiswhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    audio_encoder,\n",
        "    text_encoder,\n",
        "    loss_function,\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    batch_size=8,\n",
        "    num_epochs=30,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=1e-5,\n",
        "    patience=5,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "):\n",
        "    \"\"\"\n",
        "    Universal training function for any audio encoder and dataset.\n",
        "\n",
        "    Args:\n",
        "        audio_encoder: Any audio encoder model\n",
        "        text_encoder: Text encoder model (frozen)\n",
        "        loss_function: Loss function to use\n",
        "        train_dataset: Training dataset (any type)\n",
        "        val_dataset: Validation dataset (any type)\n",
        "        batch_size: Batch size\n",
        "        num_epochs: Maximum number of epochs\n",
        "        learning_rate: Learning rate\n",
        "        weight_decay: Weight decay for optimizer\n",
        "        patience: Early stopping patience\n",
        "        device: Device to train on\n",
        "\n",
        "    Returns:\n",
        "        best_model: The audio encoder with the best validation performance\n",
        "        history: Dictionary containing training and validation losses for each epoch\n",
        "    \"\"\"\n",
        "    # Move models to device\n",
        "    audio_encoder = audio_encoder.to(device)\n",
        "    text_encoder = text_encoder.to(device)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        collate_fn=train_dataset.collate_fn if hasattr(train_dataset, 'collate_fn') else None\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        collate_fn=val_dataset.collate_fn if hasattr(val_dataset, 'collate_fn') else None\n",
        "    )\n",
        "\n",
        "    # Set up optimizer (only for audio encoder as text encoder is frozen)\n",
        "    optimizer = optim.AdamW(\n",
        "        audio_encoder.parameters(),\n",
        "        lr=learning_rate,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    # Early stopping variables\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    patience_counter = 0\n",
        "\n",
        "    # History dictionary to track losses\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'epochs': []\n",
        "    }\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        # Training phase\n",
        "        audio_encoder.train()\n",
        "        text_encoder.eval()  # Text encoder is always in eval mode since it's frozen\n",
        "\n",
        "        train_losses = []\n",
        "\n",
        "        for batch in train_loader:\n",
        "            # Extract batch data\n",
        "            audio_inputs = batch['features'].to(device)\n",
        "            captions = batch['captions']\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass - automatically handle different input formats\n",
        "            audio_embeddings = audio_encoder(audio_inputs)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                text_embeddings = text_encoder(captions)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_function(audio_embeddings, text_embeddings)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        # Calculate average training loss\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "\n",
        "        # Validation phase\n",
        "        audio_encoder.eval()\n",
        "        val_losses = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                # Extract batch data\n",
        "                audio_inputs = batch['features'].to(device)\n",
        "                captions = batch['captions']\n",
        "\n",
        "                # Forward pass - automatically handle different input formats\n",
        "                audio_embeddings = audio_encoder(audio_inputs)\n",
        "                text_embeddings = text_encoder(captions)\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = loss_function(audio_embeddings, text_embeddings)\n",
        "\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "        # Calculate average validation loss\n",
        "        avg_val_loss = np.mean(val_losses)\n",
        "\n",
        "        # Add to history\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['epochs'].append(epoch)\n",
        "\n",
        "        # Print formatted output for this epoch\n",
        "        print(f\"Epoch {epoch}: Train Loss: {avg_train_loss:.4f} Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Check for improvement\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_model_state = audio_encoder.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch} epochs\")\n",
        "            break\n",
        "\n",
        "    # Load best model state\n",
        "    audio_encoder.load_state_dict(best_model_state)\n",
        "\n",
        "    return audio_encoder, history"
      ],
      "metadata": {
        "id": "MhcHskdhsy5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_retrieval(\n",
        "    audio_encoder,\n",
        "    text_encoder,\n",
        "    test_dataset,\n",
        "    device,\n",
        "    batch_size=8,\n",
        "    top_k=10\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluation function for retrieval metrics.\n",
        "\n",
        "    Args:\n",
        "        audio_encoder: Trained audio encoder model\n",
        "        text_encoder: Text encoder model\n",
        "        test_dataset: Test/evaluation dataset\n",
        "        device: Device to run evaluation on\n",
        "        batch_size: Batch size for encoding\n",
        "        top_k: Number of top results to consider\n",
        "\n",
        "    Returns:\n",
        "        metrics: Dictionary with R@1, R@5, R@10, and mAP@10 scores\n",
        "    \"\"\"\n",
        "    audio_encoder.eval()\n",
        "    text_encoder.eval()\n",
        "\n",
        "    # First pass: encode all audio files\n",
        "    audio_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        collate_fn=test_dataset.collate_fn if hasattr(test_dataset, 'collate_fn') else None\n",
        "    )\n",
        "\n",
        "    # Collect all audio embeddings and file names\n",
        "    all_audio_embeds = []\n",
        "    all_file_names = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in audio_loader:\n",
        "            audio_inputs = batch['features'].to(device)\n",
        "            file_names = batch['file_names']\n",
        "\n",
        "            # Forward pass - automatically handle different input formats\n",
        "            audio_embed = audio_encoder(audio_inputs)\n",
        "\n",
        "            all_audio_embeds.append(audio_embed.cpu())\n",
        "            all_file_names.extend(file_names)\n",
        "\n",
        "    # Stack all audio embeddings\n",
        "    all_audio_embeds = torch.cat(all_audio_embeds, dim=0).to(device)\n",
        "\n",
        "    # Create a dataloader with batch size 1 to handle individual captions\n",
        "    query_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        collate_fn=test_dataset.collate_fn if hasattr(test_dataset, 'collate_fn') else None\n",
        "    )\n",
        "\n",
        "    # Initialize metrics\n",
        "    recalls = {1: [], 5: [], 10: []}\n",
        "    aps = []\n",
        "\n",
        "    # For each query, get the top-k closest audio files\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(query_loader):\n",
        "\n",
        "            query_caption = batch['captions'][0]\n",
        "            ground_truth_file = batch['file_names'][0]\n",
        "\n",
        "            # Encode text query\n",
        "            text_embed = text_encoder([query_caption]).to(device)\n",
        "\n",
        "            # Calculate similarities with all audio files\n",
        "            similarities = torch.matmul(text_embed, all_audio_embeds.T)[0]\n",
        "\n",
        "            # Get indices of top-k most similar audios\n",
        "            _, top_indices = torch.topk(similarities, k=min(top_k, len(all_audio_embeds)))\n",
        "\n",
        "            # Convert to CPU for processing\n",
        "            top_indices = top_indices.cpu().numpy()\n",
        "\n",
        "            # Calculate recall metrics\n",
        "            for k in [1, 5, 10]:\n",
        "                if k <= len(top_indices):\n",
        "                    # Check if ground truth is in top-k\n",
        "                    top_k_files = [all_file_names[idx] for idx in top_indices[:k]]\n",
        "                    recall_k = 1.0 if ground_truth_file in top_k_files else 0.0\n",
        "                    recalls[k].append(recall_k)\n",
        "\n",
        "            # Calculate Average Precision for mAP\n",
        "            ap = 0.0\n",
        "            relevant_count = 0\n",
        "\n",
        "            for j, idx in enumerate(top_indices[:top_k]):\n",
        "                if all_file_names[idx] == ground_truth_file:\n",
        "                    relevant_count += 1\n",
        "                    # Precision at position j+1\n",
        "                    precision_at_j = relevant_count / (j + 1)\n",
        "                    ap += precision_at_j\n",
        "\n",
        "            # If the relevant item is in top_k, divide by 1, otherwise AP is 0\n",
        "            if relevant_count > 0:\n",
        "                ap /= relevant_count\n",
        "\n",
        "            aps.append(ap)\n",
        "\n",
        "    # Calculate final metrics\n",
        "    r1 = sum(recalls[1]) / len(recalls[1]) if recalls[1] else 0\n",
        "    r5 = sum(recalls[5]) / len(recalls[5]) if recalls[5] else 0\n",
        "    r10 = sum(recalls[10]) / len(recalls[10]) if recalls[10] else 0\n",
        "    map10 = sum(aps) / len(aps) if aps else 0\n",
        "\n",
        "    metrics = {\n",
        "        'R@1': r1,\n",
        "        'R@5': r5,\n",
        "        'R@10': r10,\n",
        "        'mAP@10': map10\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "E9iLzovCyW9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runs"
      ],
      "metadata": {
        "id": "2niBiNDftBQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "base_dir = \"/content\"\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 8\n",
        "learning_rate = 1e-4\n",
        "weight_decay = 1e-5\n",
        "num_epochs = 100\n",
        "patience = 5\n",
        "embedding_dim = 512\n",
        "\n",
        "augmentations = ['time_stretch', 'pitch_shift', 'add_noise', 'time_mask', 'freq_mask']\n",
        "\n",
        "all_histories = {}\n",
        "\n",
        "text_encoder = BertTextEncoder(\n",
        "    bert_model_name=\"bert-base-uncased\",\n",
        "    output_dim=embedding_dim,\n",
        "    pooling_strategy=\"cls\"\n",
        ").to(device)\n",
        "\n",
        "standard_train_dataset = LogMelSpectrogramDataset(\n",
        "    base_dir=base_dir,\n",
        "    split='train',\n",
        "    max_length_seconds=30,\n",
        "    augmentations=augmentations\n",
        ")\n",
        "\n",
        "standard_val_dataset = LogMelSpectrogramDataset(\n",
        "    base_dir=base_dir,\n",
        "    split='eval',\n",
        "    max_length_seconds=30,\n",
        "    augmentations=None\n",
        ")\n",
        "\n",
        "multiscale_train_dataset = MultiscaleLogMelDataset(\n",
        "    base_dir=base_dir,\n",
        "    split='train',\n",
        "    max_length_seconds=30,\n",
        "    n_fft_scales=[512, 1024, 2048],\n",
        "    hop_length_scales=[256, 512, 1024],\n",
        "    augmentations=augmentations\n",
        ")\n",
        "\n",
        "multiscale_val_dataset = MultiscaleLogMelDataset(\n",
        "    base_dir=base_dir,\n",
        "    split='eval',\n",
        "    max_length_seconds=30,\n",
        "    n_fft_scales=[512, 1024, 2048],\n",
        "    hop_length_scales=[256, 512, 1024],\n",
        "    augmentations=None\n",
        ")"
      ],
      "metadata": {
        "id": "N2GEdjjsxVSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Training with InfoNCE Loss and Standard Mel Spectrograms ===\\n\")\n",
        "\n",
        "# Initialize audio encoder\n",
        "audio_encoder = AudioEncoderCRNN(\n",
        "    output_dim=embedding_dim,\n",
        "    cnn_channels=[64, 128, 256, 512],\n",
        "    kernel_size=3,\n",
        "    stride=2,\n",
        "    gru_hidden_size=512,\n",
        "    gru_num_layers=2,\n",
        "    dropout_rate=0.3,\n",
        "    use_multiscale=False\n",
        ").to(device)\n",
        "\n",
        "# Initialize loss function\n",
        "loss_function = InfoNCELoss(temperature=0.07)\n",
        "\n",
        "# Train model\n",
        "best_model, history = train_model(\n",
        "    audio_encoder=audio_encoder,\n",
        "    text_encoder=text_encoder,\n",
        "    loss_function=loss_function,\n",
        "    train_dataset=standard_train_dataset,\n",
        "    val_dataset=standard_val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=num_epochs,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    patience=patience,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Store history\n",
        "all_histories['infonce_standard'] = history\n",
        "\n",
        "# Evaluate retrieval performance\n",
        "metrics = evaluate_retrieval(best_model, text_encoder, standard_val_dataset, device)\n",
        "print(\"\\nRetrieval Metrics:\")\n",
        "print(f\"R@1: {metrics['R@1']:.4f}, R@5: {metrics['R@5']:.4f}, R@10: {metrics['R@10']:.4f}, mAP@10: {metrics['mAP@10']:.4f}\")\n",
        "\n",
        "# Add metrics to history\n",
        "all_histories['infonce_standard']['metrics'] = metrics"
      ],
      "metadata": {
        "id": "qKfhZB2HzEai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Training with InfoNCE Loss and Multiscale Mel Spectrograms ===\\n\")\n",
        "\n",
        "# Initialize audio encoder\n",
        "audio_encoder = AudioEncoderCRNN(\n",
        "    output_dim=embedding_dim,\n",
        "    cnn_channels=[64, 128, 256, 512],\n",
        "    kernel_size=3,\n",
        "    stride=2,\n",
        "    gru_hidden_size=512,\n",
        "    gru_num_layers=2,\n",
        "    dropout_rate=0.3,\n",
        "    use_multiscale=True\n",
        ").to(device)\n",
        "\n",
        "# Initialize loss function\n",
        "loss_function = InfoNCELoss(temperature=0.07)\n",
        "\n",
        "# Train model\n",
        "best_model, history = train_model(\n",
        "    audio_encoder=audio_encoder,\n",
        "    text_encoder=text_encoder,\n",
        "    loss_function=loss_function,\n",
        "    train_dataset=multiscale_train_dataset,\n",
        "    val_dataset=multiscale_val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=num_epochs,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    patience=patience,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Store history\n",
        "all_histories['infonce_multiscale'] = history\n",
        "\n",
        "# Evaluate retrieval performance\n",
        "metrics = evaluate_retrieval(best_model, text_encoder, multiscale_val_dataset, device)\n",
        "print(\"\\nRetrieval Metrics:\")\n",
        "print(f\"R@1: {metrics['R@1']:.4f}, R@5: {metrics['R@5']:.4f}, R@10: {metrics['R@10']:.4f}, mAP@10: {metrics['mAP@10']:.4f}\")\n",
        "\n",
        "# Add metrics to history\n",
        "all_histories['infonce_multiscale']['metrics'] = metrics"
      ],
      "metadata": {
        "id": "kUMGPme0subF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Training with VICReg Loss and Standard Mel Spectrograms ===\\n\")\n",
        "\n",
        "# Initialize audio encoder\n",
        "audio_encoder = AudioEncoderCRNN(\n",
        "    output_dim=embedding_dim,\n",
        "    cnn_channels=[64, 128, 256, 512],\n",
        "    kernel_size=3,\n",
        "    stride=2,\n",
        "    gru_hidden_size=512,\n",
        "    gru_num_layers=2,\n",
        "    dropout_rate=0.3,\n",
        "    use_multiscale=False\n",
        ").to(device)\n",
        "\n",
        "# Initialize loss function\n",
        "loss_function = VICRegLoss(sim_weight=25.0, var_weight=25.0, cov_weight=1.0)\n",
        "\n",
        "# Train model\n",
        "best_model, history = train_model(\n",
        "    audio_encoder=audio_encoder,\n",
        "    text_encoder=text_encoder,\n",
        "    loss_function=loss_function,\n",
        "    train_dataset=standard_train_dataset,\n",
        "    val_dataset=standard_val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=num_epochs,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    patience=patience,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Store history\n",
        "all_histories['vicreg_standard'] = history\n",
        "\n",
        "# Evaluate retrieval performance\n",
        "metrics = evaluate_retrieval(best_model, text_encoder, standard_val_dataset, device)\n",
        "print(\"\\nRetrieval Metrics:\")\n",
        "print(f\"R@1: {metrics['R@1']:.4f}, R@5: {metrics['R@5']:.4f}, R@10: {metrics['R@10']:.4f}, mAP@10: {metrics['mAP@10']:.4f}\")\n",
        "\n",
        "# Add metrics to history\n",
        "all_histories['vicreg_standard']['metrics'] = metrics"
      ],
      "metadata": {
        "id": "DgJ64d_CyJWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Training with VICReg Loss and Multiscale Mel Spectrograms ===\\n\")\n",
        "\n",
        "# Initialize audio encoder\n",
        "audio_encoder = AudioEncoderCRNN(\n",
        "    output_dim=embedding_dim,\n",
        "    cnn_channels=[64, 128, 256, 512],\n",
        "    kernel_size=3,\n",
        "    stride=2,\n",
        "    gru_hidden_size=512,\n",
        "    gru_num_layers=2,\n",
        "    dropout_rate=0.3,\n",
        "    use_multiscale=True\n",
        ").to(device)\n",
        "\n",
        "# Initialize loss function\n",
        "loss_function = VICRegLoss(sim_weight=25.0, var_weight=25.0, cov_weight=1.0)\n",
        "\n",
        "# Train model\n",
        "best_model, history = train_model(\n",
        "    audio_encoder=audio_encoder,\n",
        "    text_encoder=text_encoder,\n",
        "    loss_function=loss_function,\n",
        "    train_dataset=multiscale_train_dataset,\n",
        "    val_dataset=multiscale_val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=num_epochs,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    patience=patience,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Store history\n",
        "all_histories['vicreg_multiscale'] = history\n",
        "\n",
        "# Evaluate retrieval performance\n",
        "metrics = evaluate_retrieval(best_model, text_encoder, multiscale_val_dataset, device)\n",
        "print(\"\\nRetrieval Metrics:\")\n",
        "print(f\"R@1: {metrics['R@1']:.4f}, R@5: {metrics['R@5']:.4f}, R@10: {metrics['R@10']:.4f}, mAP@10: {metrics['mAP@10']:.4f}\")\n",
        "\n",
        "# Add metrics to history\n",
        "all_histories['vicreg_multiscale']['metrics'] = metrics"
      ],
      "metadata": {
        "id": "WmMSd-aEyPL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Training with Cosine Loss and Standard Mel Spectrograms ===\\n\")\n",
        "\n",
        "# Initialize audio encoder\n",
        "audio_encoder = AudioEncoderCRNN(\n",
        "    output_dim=embedding_dim,\n",
        "    cnn_channels=[64, 128, 256, 512],\n",
        "    kernel_size=3,\n",
        "    stride=2,\n",
        "    gru_hidden_size=512,\n",
        "    gru_num_layers=2,\n",
        "    dropout_rate=0.3,\n",
        "    use_multiscale=False\n",
        ").to(device)\n",
        "\n",
        "# Initialize loss function\n",
        "loss_function = CosineLoss()\n",
        "\n",
        "# Train model\n",
        "best_model, history = train_model(\n",
        "    audio_encoder=audio_encoder,\n",
        "    text_encoder=text_encoder,\n",
        "    loss_function=loss_function,\n",
        "    train_dataset=standard_train_dataset,\n",
        "    val_dataset=standard_val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=num_epochs,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    patience=patience,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Store history\n",
        "all_histories['cosine_standard'] = history\n",
        "\n",
        "# Evaluate retrieval performance\n",
        "metrics = evaluate_retrieval(best_model, text_encoder, standard_val_dataset, device)\n",
        "print(\"\\nRetrieval Metrics:\")\n",
        "print(f\"R@1: {metrics['R@1']:.4f}, R@5: {metrics['R@5']:.4f}, R@10: {metrics['R@10']:.4f}, mAP@10: {metrics['mAP@10']:.4f}\")\n",
        "\n",
        "# Add metrics to history\n",
        "all_histories['cosine_standard']['metrics'] = metrics"
      ],
      "metadata": {
        "id": "HNlRC-J3ySH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Training with Cosine Loss and Multiscale Mel Spectrograms ===\\n\")\n",
        "\n",
        "# Initialize audio encoder\n",
        "audio_encoder = AudioEncoderCRNN(\n",
        "    output_dim=embedding_dim,\n",
        "    cnn_channels=[64, 128, 256, 512],\n",
        "    kernel_size=3,\n",
        "    stride=2,\n",
        "    gru_hidden_size=512,\n",
        "    gru_num_layers=2,\n",
        "    dropout_rate=0.3,\n",
        "    use_multiscale=True\n",
        ").to(device)\n",
        "\n",
        "# Initialize loss function\n",
        "loss_function = CosineLoss()\n",
        "\n",
        "# Train model\n",
        "best_model, history = train_model(\n",
        "    audio_encoder=audio_encoder,\n",
        "    text_encoder=text_encoder,\n",
        "    loss_function=loss_function,\n",
        "    train_dataset=multiscale_train_dataset,\n",
        "    val_dataset=multiscale_val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=num_epochs,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    patience=patience,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Store history\n",
        "all_histories['cosine_multiscale'] = history\n",
        "\n",
        "# Evaluate retrieval performance\n",
        "metrics = evaluate_retrieval(best_model, text_encoder, multiscale_val_dataset, device)\n",
        "print(\"\\nRetrieval Metrics:\")\n",
        "print(f\"R@1: {metrics['R@1']:.4f}, R@5: {metrics['R@5']:.4f}, R@10: {metrics['R@10']:.4f}, mAP@10: {metrics['mAP@10']:.4f}\")\n",
        "\n",
        "# Add metrics to history\n",
        "all_histories['cosine_multiscale']['metrics'] = metrics"
      ],
      "metadata": {
        "id": "Eddr3u1RyfGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting function\n",
        "def plot_training_histories(histories, save_path='training_results.png'):\n",
        "    \"\"\"\n",
        "    Plot training results in a 3x2 grid (3 loss functions  2 feature types)\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    fig.suptitle('Training Results for Different Loss Functions and Feature Types', fontsize=16)\n",
        "\n",
        "    # Define positions and titles\n",
        "    positions = {\n",
        "        'infonce_standard': (0, 0),\n",
        "        'infonce_multiscale': (1, 0),\n",
        "        'vicreg_standard': (0, 1),\n",
        "        'vicreg_multiscale': (1, 1),\n",
        "        'cosine_standard': (0, 2),\n",
        "        'cosine_multiscale': (1, 2)\n",
        "    }\n",
        "\n",
        "    titles = {\n",
        "        'infonce_standard': 'InfoNCE - Standard Mel',\n",
        "        'infonce_multiscale': 'InfoNCE - Multiscale Mel',\n",
        "        'vicreg_standard': 'VICReg - Standard Mel',\n",
        "        'vicreg_multiscale': 'VICReg - Multiscale Mel',\n",
        "        'cosine_standard': 'Cosine - Standard Mel',\n",
        "        'cosine_multiscale': 'Cosine - Multiscale Mel'\n",
        "    }\n",
        "\n",
        "    # Plot each history\n",
        "    for key, history in histories.items():\n",
        "        if key in positions:\n",
        "            row, col = positions[key]\n",
        "            ax = axes[row, col]\n",
        "\n",
        "            # Plot loss curves\n",
        "            ax.plot(history['epochs'], history['train_loss'], label='Training Loss', marker='o')\n",
        "            ax.plot(history['epochs'], history['val_loss'], label='Validation Loss', marker='x')\n",
        "\n",
        "            # Add title and labels\n",
        "            ax.set_title(titles[key])\n",
        "            ax.set_xlabel('Epoch')\n",
        "            ax.set_ylabel('Loss')\n",
        "            ax.legend()\n",
        "            ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "            # Add metrics as text if available\n",
        "            if 'metrics' in history:\n",
        "                metrics_text = (f\"R@1: {history['metrics']['R@1']:.3f}\\n\"\n",
        "                               f\"R@5: {history['metrics']['R@5']:.3f}\\n\"\n",
        "                               f\"R@10: {history['metrics']['R@10']:.3f}\\n\"\n",
        "                               f\"mAP@10: {history['metrics']['mAP@10']:.3f}\")\n",
        "                ax.text(0.05, 0.95, metrics_text, transform=ax.transAxes, fontsize=9,\n",
        "                        verticalalignment='top', bbox=dict(boxstyle='round', alpha=0.1))\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for the suptitle\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Plot all results\n",
        "plot_training_histories(all_histories)"
      ],
      "metadata": {
        "id": "LWOf3tanynzi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}